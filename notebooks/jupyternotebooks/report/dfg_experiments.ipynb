{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scripts.components.component_keyword import add_entity_ruler\n",
    "import scripts.component_lowercase_lemmas\n",
    "\n",
    "# for ._.domain_label extension!\n",
    "from scripts.components import predict_domain_from_keywords\n",
    "from scripts.data_helpers import extract_sentence_entities\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"lower_case_lemmas\", after=\"lemmatizer\")\n",
    "nlp.remove_pipe(\"ner\")\n",
    "ruler = add_entity_ruler(nlp)\n",
    "nlp.add_pipe(\"domain_classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.components import financial_tone_classifier\n",
    "from scripts.components import sustainability_potential_classifier\n",
    "\n",
    "nlp.add_pipe(\"financial_tone_classifier\")\n",
    "nlp.add_pipe(\"sustainability_potential_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Building operations are responsible for 41.7% of U.S. energy consumption, with building construction and materials accounting for an additional 5.9% of consumption, as shown in Fig. 1. Over the past decades, designers have become more aware of the need to conserve natural resources, reduce energy use, and minimise carbon pollution. The strategies have primarily focused on reducing energy use from carbon emitting sources during the operation of buildings, but this is only part of the carbon emissions story.')\n",
    "print([[(ent, ent.label_) for ent in sent.ents] for sent in doc.sents])\n",
    "print([sent._.labels for sent in doc.sents])\n",
    "print(doc._.labels)\n",
    "print()\n",
    "print(extract_sentence_entities(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "banter_paragraphs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "from tqdm.notebook import tqdm\n",
    "from scripts.components.component_keyword import ID_TO_LABEL\n",
    "\n",
    "items = srsly.read_jsonl(\"corpus/paragraphs/paragraphs.jsonl\")\n",
    "\n",
    "pbar = tqdm(\n",
    "    enumerate(\n",
    "        nlp.pipe([(item[\"text\"], item) for i, item in enumerate(items)], as_tuples=True)\n",
    "    )\n",
    ")\n",
    "for par_i, (doc, item) in pbar:\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    if len(sentences) < 3:\n",
    "        continue\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    if not (ent for ent in doc.ents if ent.label_ in nlp.pipe):\n",
    "        continue\n",
    "\n",
    "    paragraph_domain = doc._.labels['domain']\n",
    "    \n",
    "    paragraph = item | {\n",
    "        \"index\": par_i,\n",
    "        \"domain\": paragraph_domain,\n",
    "        \"sents\": [sent.text for sent in sentences],\n",
    "    }\n",
    "\n",
    "    if (paragraph_domain == \"ENV\"):\n",
    "        paragraphs.append(paragraph)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_json(doc):\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    paragraph_domain = doc._.labels['domain']\n",
    "\n",
    "    paragraph = {\n",
    "        \"index\": par_i,\n",
    "        **doc._.labels,\n",
    "        \"sents\": [sent.text for sent in sentences],\n",
    "        \"entities\": extract_sentence_entities(doc),\n",
    "        \"sent_labels\": [{'sent_ind':i, **sent._.labels} for i, sent in enumerate(sentences)]\n",
    "    }\n",
    "    return paragraph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_components=[\"entity_ruler\"]\n",
    "[','.join(set([ent.text for ent in doc.ents if ent.label_ == label])) for comp in keyword_components for label in nlp.pipe_labels[comp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[label for comp in keyword_components for label in nlp.pipe_labels[comp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from typing import List, Iterable\n",
    "\n",
    "\n",
    "def documents_to_tsv(\n",
    "    out_path: str,\n",
    "    docs_items: Iterable[Doc],\n",
    "    keyword_components=[\"entity_ruler\"],\n",
    "    classifiers=[\"financial_tone\"],\n",
    "):\n",
    "    HEADER = [\n",
    "        \"SCRAPER\",\n",
    "        \"TITLE\",\n",
    "        \"URL\",\n",
    "        \"PARAGRAPH_INDEX\",\n",
    "        *[\"KEYWORDS_\" + label for comp in keyword_components\n",
    "                    for label in nlp.pipe_labels[comp]],\n",
    "        *[classifier for classifier in classifiers],\n",
    "        \"SENTENCE\",\n",
    "    ]\n",
    "    with open(out_path, \"wt\") as file:\n",
    "        file.write(\"\\t\".join(HEADER) + \"\\n\")\n",
    "        for doc, item in docs_items:\n",
    "            par_line = [\n",
    "                item[\"scraper\"],\n",
    "                item[\"title\"],\n",
    "                item[\"url\"],\n",
    "                str(item[\"par_index\"]),\n",
    "                *[\n",
    "                    \",\".join(set([ent.text for ent in doc.ents if ent.label_ == label]))\n",
    "                    for comp in keyword_components\n",
    "                    for label in nlp.pipe_labels[comp]\n",
    "                ],\n",
    "                *[doc._.labels.get(cls, '') for cls in classifiers],\n",
    "            ]\n",
    "            par_line = \"\\t\".join(par_line) + '\\n'\n",
    "            file.write(par_line)\n",
    "            for sent in doc.sents:\n",
    "                sent_line = [\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    *[\n",
    "                        \",\".join(\n",
    "                            set([ent.text for ent in sent.ents if ent.label_ == label])\n",
    "                        )\n",
    "                        for comp in keyword_components\n",
    "                        for label in nlp.pipe_labels[comp]\n",
    "                    ],\n",
    "                    *[sent._.labels.get(cls, '') for cls in classifiers],\n",
    "                    sent.text,\n",
    "                ]\n",
    "                sent_line = \"\\t\".join(sent_line) + '\\n'\n",
    "                file.write(sent_line)\n",
    "            file.write(\"\\n\")\n",
    "            yield (doc, item)\n",
    "\n",
    "\n",
    "# docs_items = nlp.pipe([(item[\"text\"], item) for i, item in enumerate(items)], as_tuples=True)\n",
    "# docs_items = documents_to_tsv(\"14012022_labeled_sentences.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_items = documents_to_tsv(\"14012022_labeled_sentences.tsv\", [(doc, item)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc.sents)[2]._.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in docs_items:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(env_sentences)\n",
    "df.to_csv('env_sentences.csv')\n",
    "df = pd.DataFrame(social_sentences)\n",
    "df.to_csv('social_sentences.csv')\n",
    "df = pd.DataFrame(banter_sentences)\n",
    "df.to_csv('banter_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('notebooks/sentences.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['text', 'sentiment']).agg(lambda x: list(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['sentiment'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (st.CorpusFromPandas(df[df['sentiment'] != 'neutral'],\n",
    "                              category_col='sentiment',\n",
    "                              text_col='text',\n",
    "                              nlp=st.whitespace_nlp_with_sentences)\n",
    "          .build()\n",
    "          .get_stoplisted_unigram_corpus()\n",
    "          .compact(st.ClassPercentageCompactor(term_count=2,\n",
    "                                               term_ranker=st.OncePerDocFrequencyRanker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_characteristic_explorer(\n",
    "\tcorpus,\n",
    "\tcategory='positive',\n",
    "\tcategory_name='positive',\n",
    "\tnot_category_name='negative',\n",
    ")\n",
    "open('demo_characteristic_chart.html', 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = st.SampleCorpora.ConventionData2012.get_data().assign(\n",
    "    parse=lambda df: df.text.apply(st.whitespace_nlp_with_sentences)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50ef1f00e07f65fb8c66231f14f6a624080f70ef226807e0f66598eceb63f363"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('smart-evidence-0MftrBAc-py3.9': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
