{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.chdir('../..')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/gos02501e/.cache/pypoetry/virtualenvs/smart-evidence-My4wO2kg-py3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from smart_evidence.components.company_impact_classifier import CompanyImpactClassifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading: 100%|██████████| 2.95k/2.95k [00:00<00:00, 1.40MB/s]\n",
                        "/home/gos02501e/.cache/pypoetry/virtualenvs/smart-evidence-My4wO2kg-py3.8/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "clf = CompanyImpactClassifier()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import srsly\n",
                "documents = list(srsly.read_jsonl('data/impact_polarity.jsonl'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def translate_concept(concept):\n",
                "    label = concept.pop('concept_label')\n",
                "    return {**concept, 'label': label}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "documents = [{'text': d['text'], \n",
                "    'meta': {\n",
                "        'predictions': {'concepts':{'annotation': {'company_concepts': [translate_concept(d['concept_relation']['company_concept'])],\n",
                "        \"impact_concepts\": [translate_concept(d['concept_relation']['impact_concept'])]}}},\n",
                "    }, \n",
                "    'annotation': ''.join(d['accept'])} for d in documents if d['answer'] == 'accept']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  9%|▉         | 10/110 [00:01<00:18,  5.44it/s]/home/gos02501e/.cache/pypoetry/virtualenvs/smart-evidence-My4wO2kg-py3.8/lib/python3.8/site-packages/transformers/pipelines/base.py:1012: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
                        "  warnings.warn(\n",
                        "100%|██████████| 110/110 [00:15<00:00,  6.89it/s]\n"
                    ]
                }
            ],
            "source": [
                "annotated_documents = clf.run(documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'company_concept': {'label': 'Cement',\n",
                            "   'id': 'http://dbpedia.org/resource/Cement'},\n",
                            "  'impact_concept': {'label': 'Recycling',\n",
                            "   'id': 'http://dbpedia.org/resource/Recycling'},\n",
                            "  'label': <ImpactPolarity.positive: 'POSITIVE'>}]"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "annotated_documents[0]['meta']['predictions']['relations']['annotation']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "LABEL_TRANSLATION = {\n",
                "    \"NEGATIVE\": \"NEGATIVE\",\n",
                "    \"POSITIVE\": \"POSITIVE\",\n",
                "    \"NOT_RELATED\": \"NOT_RELATED\",\n",
                "    \"POSITIVE_CONTRADICTION\": \"NOT_RELATED\",\n",
                "    \"NEGATIVE_CONTRADICTION\": \"NOT_RELATED\",\n",
                "    \"CONTRADICTION\": \"NOT_RELATED\"\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(\n",
                "    [\n",
                "        (\n",
                "            d[\"text\"],\n",
                "            d[\"meta\"]['predictions']['relations']['annotation'][0][\"company_concept\"][\"label\"],\n",
                "            d[\"meta\"]['predictions']['relations']['annotation'][0][\"impact_concept\"][\"label\"],\n",
                "            LABEL_TRANSLATION[d[\"meta\"]['predictions']['relations']['annotation'][0][\"label\"].value],\n",
                "            d[\"annotation\"],\n",
                "        )\n",
                "        for d in annotated_documents\n",
                "    ],\n",
                "    columns=[\"text\", \"company_concept\", \"impact_concept\", \"prediction\", \"annotation\"],\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'There is a medium level of evidence and agreement on the benefit of recycling of construction minerals, with high agreement that existing recycling as aggregates reduces the energy demand associated with aggregate production, but limited evidence for the benefit of recycling cement or concrete to anything but aggregate. There is insufficient evidence to evaluate the suitability of recycling of construction minerals and plastics under future conditions of a more stringent emissions control policy.'"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "documents[0]['text'][548:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample = documents[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/gos02501e/.cache/pypoetry/virtualenvs/smart-evidence-My4wO2kg-py3.8/lib/python3.8/site-packages/transformers/pipelines/base.py:1012: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[{'company_concept': {'id': 'http://dbpedia.org/resource/Cement',\n",
                            "   'label': 'Cement'},\n",
                            "  'impact_concept': {'id': 'http://dbpedia.org/resource/Recycling',\n",
                            "   'label': 'Recycling'},\n",
                            "  'label': 'POSITIVE'}]"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "clf.process_result_to_meta(sample['text'][548:], sample['meta']['predictions']['concepts']['annotation']['company_concepts'], sample['meta']['predictions']['concepts']['annotation']['impact_concepts'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['is_correct'] = df['annotation'] == df['prediction']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample = df[False == df['is_correct']].loc[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'text': 'There is a medium level of evidence and a high level of agreement that the recycling of metals from buildings and vehicles already contributes to substantial emission reductions, while the recycling of EEE addresses other environmental concerns but contributes little to overall GHG mitigation. There is a limited level of evidence but agreement that further emission reductions can be achieved by sorting metals according to alloys to avoid the contamination of metal flows and allow for recycling even when metal stocks are no longer increasing. There is a medium level of evidence and agreement on the benefit of recycling of construction minerals, with high agreement that existing recycling as aggregates reduces the energy demand associated with aggregate production, but limited evidence for the benefit of recycling cement or concrete to anything but aggregate. There is insufficient evidence to evaluate the suitability of recycling of construction minerals and plastics under future conditions of a more stringent emissions control policy.',\n",
                            " 'company_concept': 'Cement',\n",
                            " 'impact_concept': 'Recycling',\n",
                            " 'prediction': 'POSITIVE',\n",
                            " 'annotation': 'NEGATIVE',\n",
                            " 'is_correct': False}"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dict(sample)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "NOT_RELATED    58\n",
                            "NEGATIVE       30\n",
                            "POSITIVE       22\n",
                            "Name: annotation, dtype: int64"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df['annotation'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "NOT_RELATED    55\n",
                            "POSITIVE       30\n",
                            "NEGATIVE       25\n",
                            "Name: prediction, dtype: int64"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df['prediction'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    NEGATIVE       0.40      0.33      0.36        30\n",
                        " NOT_RELATED       0.53      0.50      0.51        58\n",
                        "    POSITIVE       0.43      0.59      0.50        22\n",
                        "\n",
                        "    accuracy                           0.47       110\n",
                        "   macro avg       0.45      0.47      0.46       110\n",
                        "weighted avg       0.47      0.47      0.47       110\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(df['annotation'], df['prediction']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "ename": "IndentationError",
                    "evalue": "unindent does not match any outer indentation level (<tokenize>, line 9)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m<tokenize>:9\u001b[0;36m\u001b[0m\n\u001b[0;31m    macro avg       0.31      0.34      0.32       110\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
                    ]
                }
            ],
            "source": [
                "                        precision    recall  f1-score   support\n",
                "\n",
                "              NEGATIVE       0.32      0.27      0.29        30\n",
                "NEGATIVE_CONTRADICTION       0.00      0.00      0.00         0\n",
                "           NOT_RELATED       0.46      0.40      0.43        58\n",
                "              POSITIVE       0.45      0.68      0.55        22\n",
                "\n",
                "              accuracy                           0.42       110\n",
                "             macro avg       0.31      0.34      0.32       110\n",
                "          weighted avg       0.42      0.42      0.41       110\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(df['annotation'], df['prediction']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(df['annotation'], df['prediction']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# NEGATIVE       0.44       0.81      0.57        36\n",
                "# NOT_RELATED    0.00       0.00      0.00        59\n",
                "# POSITIVE       0.48       0.97      0.64        29\n",
                "\n",
                "# accuracy                            0.46       124\n",
                "# macro avg       0.31      0.59      0.40       124\n",
                "# weighted avg    0.24      0.46      0.32       124"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.12 ('smart-evidence-My4wO2kg-py3.8')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "d0a532144255747c9fc026cc5fe041066dd2136635c93ea916b266688de6dfd4"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}