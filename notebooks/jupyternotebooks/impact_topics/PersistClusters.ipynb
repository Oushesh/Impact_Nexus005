{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.chdir('../..')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from haystack.nodes import RAGenerator, DensePassageRetriever\n",
                "\n",
                "dense_retriever = DensePassageRetriever(\n",
                "    document_store=paragraph_document_store,\n",
                "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
                "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
                "    use_gpu=True,\n",
                "    embed_title=True,\n",
                ")\n",
                "\n",
                "# Initialize RAG Generator\n",
                "generator = RAGenerator(\n",
                "    model_name_or_path=\"facebook/rag-token-nq\",\n",
                "    use_gpu=True,\n",
                "    top_k=1,\n",
                "    max_length=200,\n",
                "    min_length=2,\n",
                "    embed_title=True,\n",
                "    num_beams=2,\n",
                "    retriever=dense_retriever\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from rest_api.controller.impact_screening import _process_request_clusters\n",
                "from rest_api.controller.impact_topic import calculate_clusters\n",
                "from rest_api.schema import ImpactTopicRequest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = calculate_clusters(ImpactTopicRequest(impact_concept=\"Cancer\", company_concept=\"Paint\", impact_polarity=\"NEGATIVE\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "documents = response.documents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "contents = [doc.text for doc in documents]\n",
                "document_embeddings = embedder.encode(\n",
                "    contents, convert_to_tensor=True\n",
                ").cpu()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import Birch, AgglomerativeClustering\n",
                "\n",
                "agg_clustering = AgglomerativeClustering(\n",
                "    n_clusters=None, distance_threshold=0.6\n",
                "    )\n",
                "brc = Birch(threshold=0.5, n_clusters=agg_clustering)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brc.fit(document_embeddings[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brc.predict(document_embeddings[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "brc.partial_fit(document_embeddings[10:])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "labels = brc.predict(document_embeddings)\n",
                "labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from haystack.pipelines import Pipeline\n",
                "from smart_evidence.helpers import opensearch_connection\n",
                "from smart_evidence.components.count_clustering import CountClustering\n",
                "from smart_evidence.components.count_transformers_clustering import CountTransformerClustering\n",
                "from smart_evidence.components.document_classifier import HeuristicsDocumentClassifier\n",
                "from smart_evidence.components.summarizer import IXTransformersSummarizer\n",
                "from smart_evidence.components.transformers_clustering import TransformersClustering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import defaultdict, Counter\n",
                "import logging\n",
                "from typing import Any, List, Optional, Set, Tuple\n",
                "\n",
                "from haystack.nodes.base import BaseComponent\n",
                "from haystack.schema import Document\n",
                "from rest_api.config import LOG_LEVEL\n",
                "from sklearn.cluster import DBSCAN\n",
                "import numpy as np\n",
                "\n",
                "logging.getLogger(__name__).setLevel(LOG_LEVEL)\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "\n",
                "\n",
                "class TransformersClustering(BaseComponent):\n",
                "    outgoing_edges = 1\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        clustering,\n",
                "        embedder: Optional[Any] = None,\n",
                "        separator_for_cluster_texts: str = \"\\n\",\n",
                "        min_cluster_size: int = 2\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Use sklearn to vectorize and cluster documents.\n",
                "        :param separator_for_single_summary: If `generate_single_summary=True` in `predict()`, we need to join all docs\n",
                "                                             into a single text. This separator appears between those subsequent docs.\n",
                "        \"\"\"\n",
                "        self.print_log: Set[str] = set()\n",
                "        self.separator_for_cluster_texts = separator_for_cluster_texts\n",
                "        self.min_cluster_size = min_cluster_size\n",
                "        self.embedder = embedder\n",
                "        self.clustering = clustering\n",
                "\n",
                "    def run(self, documents: List[Document]):  # type: ignore\n",
                "\n",
                "        results: dict = {\n",
                "            \"documents\": [],\n",
                "            \"clusters\": [],\n",
                "            \"n_total_documents\": len(documents),\n",
                "        }\n",
                "\n",
                "        if documents:\n",
                "            (\n",
                "                results[\"documents\"],\n",
                "                results[\"clusters\"],\n",
                "            ) = self.predict(documents=documents)\n",
                "\n",
                "        return results, \"output_1\"\n",
                "\n",
                "    def cluster(self, embedding_matrix: np.ndarray) -> List[int]:\n",
                "        return list(self.clustering.fit(embedding_matrix))\n",
                "\n",
                "    def build_result(self, documents, clusters):\n",
                "        n_clusters = len(list(set(clusters)))\n",
                "        # logger.info(f\"{n_clusters} clusters for {len(documents)} documents: {clusters}\")\n",
                "\n",
                "        for document, cluster in zip(documents, clusters):\n",
                "            document.meta['cluster_id'] = cluster\n",
                "\n",
                "        return documents, clusters\n",
                "\n",
                "    def predict(\n",
                "        self,\n",
                "        documents: List[Document],\n",
                "    ) -> Tuple[List[Document], int, int]:\n",
                "        \"\"\"\n",
                "        Produce the clustering for the supplied documents.\n",
                "        These document can for example be retrieved via the Retriever.\n",
                "        :param documents: Related documents (e.g. coming from a retriever) that the answer shall be conditioned on.\n",
                "        :return: List of Documents, where Document.text contains the concatenated text of clusters and Document.meta[\"ids\"]\n",
                "                 ids of the original documents\n",
                "        \"\"\"\n",
                "        if len(documents) == 0:\n",
                "            raise AttributeError(\n",
                "                \"Summarizer needs at least one document to produce a summary.\"\n",
                "            )\n",
                "\n",
                "        contents = [doc.content for doc in documents]\n",
                "        document_embeddings = self.embedder.encode(\n",
                "            contents, convert_to_tensor=True\n",
                "        ).cpu()\n",
                "\n",
                "        clusters = self.cluster(document_embeddings)\n",
                "        result = self.build_result(documents, clusters)\n",
                "        \n",
                "        return result\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BirchTransformersClustering(TransformersClustering):\n",
                "    def cluster(self, embedding_matrix: np.ndarray) -> List[int]:\n",
                "        self.clustering.partial_fit(embedding_matrix)\n",
                "        return self.clustering.predict(embedding_matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from smart_evidence.pipeline.query_pipeline import FilterRetriever\n",
                "from smart_evidence.components.document_classifier import HeuristicsDocumentClassifier\n",
                "from smart_evidence.components.summarizer import IXTransformersSummarizer\n",
                "from haystack.pipelines import Pipeline\n",
                "\n",
                "# document_classifier = HeuristicsDocumentClassifier()\n",
                "# summarizer = IXTransformersSummarizer(\"chinhon/headline_writer\")\n",
                "\n",
                "clustering = Birch(n_clusters=None)\n",
                "transformer_clustering = BirchTransformersClustering(\n",
                "    clustering=clustering,\n",
                "    embedder=embedder, \n",
                "    min_cluster_size=2\n",
                ")\n",
                "transformer_cluster_pipeline = Pipeline()\n",
                "# transformer_cluster_pipeline.add_node(\n",
                "#     component=document_classifier, name=\"DocumentClassifier\", inputs=[\"Query\"]\n",
                "# )\n",
                "transformer_cluster_pipeline.add_node(\n",
                "    component=transformer_clustering, name=\"Clustering\", inputs=[\"Query\"]\n",
                ")\n",
                "# transformer_cluster_pipeline.add_node(\n",
                "#     component=summarizer, name=\"Summarizer\", inputs=[\"Clustering\"]\n",
                "# )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import date\n",
                "\n",
                "def write_clusters(clusters):\n",
                "    cluster_documents = []\n",
                "    for cluster_document in clusters['documents']:\n",
                "        cluster_document.meta['updated_at'] = date.today()\n",
                "        cluster_document.meta['is_curated'] = False\n",
                "        cluster_document.meta['paragraph_ids'] = [d['id'] for d in cluster_document.meta['documents']]\n",
                "        cluster_documents.append(cluster_document)\n",
                "\n",
                "    clusters_document_store.write_documents(cluster_documents)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.autonotebook import tqdm\n",
                "\n",
                "global_clusters = defaultdict(lambda: list())\n",
                "batch = []\n",
                "for i, document in tqdm(enumerate(paragraph_document_store.get_all_documents_generator())):\n",
                "    batch.append(document)\n",
                "    if len(batch) % 1000 == 0:\n",
                "        results = transformer_cluster_pipeline.run(documents=batch)\n",
                "        documents, clusters = results['documents'], results['clusters']\n",
                "        for document ,cluster_id in zip(documents, clusters):\n",
                "            global_clusters[cluster_id].append(document.id)\n",
                "        batch = []\n",
                "    # if i == 1000:\n",
                "    #     break\n",
                "\n",
                "if batch:\n",
                "    results = transformer_cluster_pipeline.run(documents=batch)\n",
                "    documents, clusters = results['documents'], results['clusters']\n",
                "    for document ,cluster_id in zip(documents, clusters):\n",
                "        global_clusters[cluster_id].append(document.id)\n",
                "    batch = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "global_clusters = dict(sorted(global_clusters.items(), key=lambda x: len(x[1]), reverse=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sum([True for k, v in global_clusters.items() if len(v) > 1]), sum([len(v) for k, v in global_clusters.items() if len(v) > 1]), sum([len(v) for k, v in global_clusters.items()])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "d0a532144255747c9fc026cc5fe041066dd2136635c93ea916b266688de6dfd4"
        },
        "kernelspec": {
            "display_name": "Python 3.8.12 ('smart-evidence-My4wO2kg-py3.8')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}